{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parser.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliewang2020/RealState/blob/master/parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApjhpNMILy18",
        "colab_type": "text"
      },
      "source": [
        "# Importing Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z2t4MSALqM-",
        "colab_type": "code",
        "outputId": "b60870c8-c6a7-4aa9-a889-5dfc15a9365c",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9c539b6d-635c-4add-8cd8-928f36f1aae2\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9c539b6d-635c-4add-8cd8-928f36f1aae2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data.csv to data (2).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS2a8YDlTbJk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2cd275c6-8bd9-4aeb-808f-783a1d3d475c"
      },
      "source": [
        "############## Cleaning the HTML\n",
        "df = pd.read_csv(io.BytesIO(uploaded['data.csv']))\n",
        "#get rid of ID and CreatedDate\n",
        "\n",
        "features = df.iloc[:, 1:6]\n",
        "#features.drop('CreatedDate', axis = 1, inplace=True)\n",
        "common_words = ['please', 'unit', 'represented','hover', 'text', 'br','said', 'office','new','million','costar', 'square', 'building', 'market', 'space', 'year', 'estate', 'real', 'align', 'development', 'feet', 'property', 'percent', 'company','foot', 'investment', 'based', 'retail', 'years', 'according']\n",
        "import re\n",
        "\n",
        "def cleanhtml(raw_html):\n",
        "  cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "  cleantext = re.sub(cleanr, '', raw_html)\n",
        "  return cleantext\n",
        "\n",
        "for i in range(0, 1933):\n",
        "  body_text = features.iloc[i , 2]\n",
        "  body_text = cleanhtml(body_text)\n",
        "  body_text = body_text.replace('\\n', ' ')\n",
        "  body_text  = [word for word in re.split(\"\\W+\",body_text) if word.lower() not in common_words]\n",
        "  body_text = ' '.join(body_text)\n",
        "  features.loc[i, 'Body'] = body_text\n",
        "print(features.shape)"
      ],
      "execution_count": 411,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1933, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7PIRJBxYhQi",
        "colab_type": "code",
        "outputId": "5573b463-1cff-4680-a128-6855776e1d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Location = []\n",
        "    \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "for i in range(0,1933):\n",
        "  title_text = features.iloc[i,0]\n",
        "  doc = nlp(title_text)\n",
        "  locations = []\n",
        "  for ent in doc.ents:\n",
        "    if ent.label_ == \"GPE\":\n",
        "      locations.append(ent.text)    \n",
        "  if not locations:\n",
        "    summary_text = features.iloc[i,1]\n",
        "    if(pd.isnull(summary_text)== False):\n",
        "      doc = nlp(summary_text)\n",
        "      locations = []\n",
        "      for ent in doc.ents:\n",
        "        if ent.label_ == \"GPE\":\n",
        "          locations.append(ent.text)\n",
        "    else:\n",
        "      body_text = features.iloc[i,2]\n",
        "      doc = nlp(body_text)\n",
        "      locations = []\n",
        "      for ent in doc.ents:\n",
        "        if ent.label_ == \"GPE\":\n",
        "          locations.append(ent.text)\n",
        "    if not locations:\n",
        "      body_text = features.iloc[i,2]\n",
        "      doc = nlp(body_text)\n",
        "      locations = []\n",
        "      for ent in doc.ents:\n",
        "        if ent.label_ == \"GPE\":\n",
        "          locations.append(ent.text)\n",
        "      Location.append(locations)        \n",
        "    else:\n",
        "      Location.append(locations)      \n",
        "  else:\n",
        "    Location.append(locations) \n",
        "features['GPE'] = Location\n",
        "print(features.shape)"
      ],
      "execution_count": 412,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1933, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTyOUL9aPys2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd0514b6-4215-4a00-dcb1-a560a07736b9"
      },
      "source": [
        "# Python3 program to find the most  \n",
        "# frequent element in an array. \n",
        "  \n",
        "def mostFrequent(arr, n): \n",
        "  \n",
        "    # Sort the array \n",
        "    arr.sort() \n",
        "  \n",
        "    # find the max frequency using \n",
        "    # linear traversal \n",
        "    max_count = 1; res = arr[0]; curr_count = 1\n",
        "      \n",
        "    for i in range(1, n):  \n",
        "        if (arr[i] == arr[i - 1]): \n",
        "            curr_count += 1\n",
        "              \n",
        "        else : \n",
        "            if (curr_count > max_count):  \n",
        "                max_count = curr_count \n",
        "                res = arr[i - 1] \n",
        "              \n",
        "            curr_count = 1\n",
        "      \n",
        "    # If last element is most frequent \n",
        "    if (curr_count > max_count): \n",
        "      \n",
        "        max_count = curr_count \n",
        "        res = arr[n - 1] \n",
        "      \n",
        "    return res \n",
        "features = features[(features['GPE'].str.len() != 0) | (features['GPE'].str.len() != 0)]\n",
        "cities_list = []\n",
        "for i in range(0, 1872):\n",
        "    cities = features.iloc[i,5]\n",
        "    n = len(cities)\n",
        "    most = mostFrequent(cities, n)\n",
        "    cities_list.append(most)\n",
        "features['final_city'] = cities_list\n",
        "\n",
        "features['final_city'] = features['final_city'].replace('SF', 'San Francisco')\n",
        "\n",
        "final_cities = ['Atlanta', \n",
        "                'Chicago', \n",
        "                'Houston', \n",
        "                'Dallas', \n",
        "                'Denver', \n",
        "                'Seattle', \n",
        "                'Phoenix', \n",
        "                'San Francisco', \n",
        "                'Los Angeles', \n",
        "                'Baltimore', \n",
        "                'Sacramento', \n",
        "                'Birmingham',\n",
        "                'Nashville', \n",
        "                'San Diego',\n",
        "                'Minneapolis',\n",
        "                'Boston']\n",
        "##### final features\n",
        "features =  features.loc[features['final_city'].isin(final_cities)]\n",
        "print(features.shape)\n"
      ],
      "execution_count": 413,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(391, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByDo4mRvAEUe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b0be6427-d6ff-42e1-d219-ae0a46fd659f"
      },
      "source": [
        "print(features.columns)\n",
        "print(features.shape)"
      ],
      "execution_count": 414,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Title', 'Summary', 'Body', 'Hits', 'AuthorID', 'GPE', 'final_city'], dtype='object')\n",
            "(391, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jipm0qrRPa72",
        "colab_type": "code",
        "outputId": "26394ddb-9242-4f49-ad98-2c605ac10b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction import text\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "eng_contractions = [\"ain't\", \"amn't\", \"aren't\", \"can't\", \"could've\", \"couldn't\",\n",
        "                    \"daresn't\", \"didn't\", \"doesn't\", \"don't\", \"gonna\", \"gotta\", \n",
        "                    \"hadn't\", \"hasn't\", \"haven't\", \"he'd\", \"he'll\", \"he's\", \"how'd\",\n",
        "                    \"how'll\", \"how's\", \"I'd\", \"I'll\", \"I'm\", \"I've\", \"isn't\", \"it'd\",\n",
        "                    \"it'll\", \"it's\", \"let's\", \"mayn't\", \"may've\", \"mightn't\", \n",
        "                    \"might've\", \"mustn't\", \"must've\", \"needn't\", \"o'clock\", \"ol'\",\n",
        "                    \"oughtn't\", \"shan't\", \"she'd\", \"she'll\", \"she's\", \"should've\",\n",
        "                    \"shouldn't\", \"somebody's\", \"someone's\", \"something's\", \"that'll\",\n",
        "                    \"that're\", \"that's\", \"that'd\", \"there'd\", \"there're\", \"there's\", \n",
        "                    \"these're\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this's\",\n",
        "                    \"those're\", \"tis\", \"twas\", \"twasn't\", \"wasn't\", \"we'd\", \"we'd've\",\n",
        "                    \"we'll\", \"we're\", \"we've\", \"weren't\", \"what'd\", \"what'll\", \n",
        "                    \"what're\", \"what's\", \"what've\", \"when's\", \"where'd\", \"where're\",\n",
        "                    \"where's\", \"where've\", \"which's\", \"who'd\", \"who'd've\", \"who'll\",\n",
        "                    \"who're\", \"who's\", \"who've\", \"why'd\", \"why're\", \"why's\", \"won't\",\n",
        "                    \"would've\", \"wouldn't\", \"y'all\", \"you'd\", \"you'll\", \"you're\", \n",
        "                    \"you've\", \"'s\", \"s\"\n",
        "                     ]\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "custom_stopwords = text.ENGLISH_STOP_WORDS.union(eng_contractions)"
      ],
      "execution_count": 415,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJf0ZbmmZdZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "def tokenize_and_stem(text, do_stem=True):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    \n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "            \n",
        "    # stem filtered tokens\n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "    \n",
        "    if do_stem:\n",
        "        return stems\n",
        "    else:\n",
        "        return filtered_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiNrntw7bCR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# not super pythonic, no, not at all.\n",
        "# use extend so it's a big flat list of vocab\n",
        "totalvocab_stemmed = []\n",
        "totalvocab_tokenized = []\n",
        "for i in range(0, 391):\n",
        "    body_text = features.iloc[i , 2]\n",
        "    allwords_stemmed = tokenize_and_stem(body_text)\n",
        "    totalvocab_stemmed.extend(allwords_stemmed)\n",
        "    \n",
        "    allwords_tokenized = tokenize_and_stem(body_text, False)\n",
        "    totalvocab_tokenized.extend(allwords_tokenized)  \n",
        "  \n",
        "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWc66FByfRij",
        "colab_type": "code",
        "outputId": "061c985a-20db-4a39-8d6b-00737797158e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#define vectorizer parameters\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
        "                                 min_df=0.2, stop_words=custom_stopwords,\n",
        "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(features['Body']) #fit the vectorizer to synopses\n",
        "\n",
        "print(tfidf_matrix.shape)\n",
        "\n",
        "terms = tfidf_vectorizer.get_feature_names()"
      ],
      "execution_count": 418,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'m\", 'abov', 'afterward', 'ai', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'ca', 'cri', 'dare', 'describ', 'did', 'doe', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'gon', 'got', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'let', 'll', 'mani', 'meanwhil', 'moreov', \"n't\", 'na', 'need', 'nobodi', 'noon', 'noth', 'nowher', 'ol', 'onc', 'onli', 'otherwis', 'ought', 'ourselv', 'perhap', 'pleas', 'sever', 'sha', 'sinc', 'sincer', 'sixti', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'ta', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 've', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'wo', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(391, 117)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K10sGl89faN-",
        "colab_type": "code",
        "outputId": "802ceec4-9ada-4ec9-f5da-13f399ffb549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import math\n",
        "\n",
        "num_clusters = 5\n",
        "\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "\n",
        "km.fit(tfidf_matrix)"
      ],
      "execution_count": 419,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
              "       n_clusters=5, n_init=10, n_jobs=None, precompute_distances='auto',\n",
              "       random_state=None, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 419
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbPkN2R_fiKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clusters = km.labels_.tolist()\n",
        "final_data = features\n",
        "final_data['cluster'] = clusters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMQhzuRDfxqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Top terms per cluster:\")\n",
        "print()\n",
        "\n",
        "#sort cluster centers by proximity to centroid\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
        "\n",
        "\n",
        "for i in range(num_clusters):\n",
        "    print(\"Cluster %d words:\" % i, end='')\n",
        "    \n",
        "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
        "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
        "    print() #add whitespace\n",
        "    print() #add whitespace\n",
        "    \n",
        "    print(\"Cluster %d titles:\" % i, end='')\n",
        "    print()\n",
        "    for title in features[features['cluster'] == i]['Title'].values.tolist():\n",
        "        print(' - %s' % title)\n",
        "    print() #add whitespace\n",
        "    print() #add whitespace\n",
        "    \n",
        "print()\n",
        "print()\n",
        "\n",
        "# from google.colab import files\n",
        "\n",
        "# final_data.to_csv('final_data.csv')\n",
        "# files.download('final_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jBAOjw9lZX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "sentiments = []\n",
        "for i in range(0, 391):\n",
        "  blob = final_data.iloc[i, 2]\n",
        "  blob = TextBlob(blob)\n",
        "  feels = blob.sentiment.polarity\n",
        "  sentiments.append(feels)\n",
        "  \n",
        "final_data['sentiments'] = sentiments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVIiG6cqpne_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# final_data.to_csv('final_data_sentiments.csv')\n",
        "# files.download('final_data_sentiments.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}