{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parser.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliewang2020/FreeRealEstate/blob/master/parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApjhpNMILy18",
        "colab_type": "text"
      },
      "source": [
        "# Importing Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z2t4MSALqM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(io.BytesIO(uploaded['data.csv']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS2a8YDlTbJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############## Cleaning the HTML\n",
        "\n",
        "#get rid of ID and CreatedDate\n",
        "df = df.loc[df['Country_USA'] == 1]\n",
        "df = df.loc[df['Tag_Company'] == 1]\n",
        "\n",
        "features = df.iloc[:, 1:]\n",
        "features.drop('CreatedDate', axis = 1, inplace=True)\n",
        "common_words = ['said', 'office','new','million','costar', 'square', 'building', 'market', 'space', 'year', 'estate', 'real', 'align', 'development', 'feet', 'property', 'percent', 'company','foot', 'investment', 'based', 'retail', 'years', 'according']\n",
        "import re\n",
        "\n",
        "def cleanhtml(raw_html):\n",
        "  cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "  cleantext = re.sub(cleanr, '', raw_html)\n",
        "  return cleantext\n",
        "\n",
        "for i in range(0, 80):\n",
        "  body_text = features.iloc[i , 2]\n",
        "  body_text = cleanhtml(body_text)\n",
        "  body_text = body_text.replace('\\n', ' ')\n",
        "  body_text  = [word for word in re.split(\"\\W+\",body_text) if word.lower() not in common_words]\n",
        "  body_text = ' '.join(body_text)\n",
        "  features.loc[i, 'Body'] = body_text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7PIRJBxYhQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.cloud import language\n",
        "from google.cloud.language import enums\n",
        "from google.cloud.language import types"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jipm0qrRPa72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "c763245f-5912-45e5-da6f-5281e34d19f0"
      },
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction import text\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "eng_contractions = [\"ain't\", \"amn't\", \"aren't\", \"can't\", \"could've\", \"couldn't\",\n",
        "                    \"daresn't\", \"didn't\", \"doesn't\", \"don't\", \"gonna\", \"gotta\", \n",
        "                    \"hadn't\", \"hasn't\", \"haven't\", \"he'd\", \"he'll\", \"he's\", \"how'd\",\n",
        "                    \"how'll\", \"how's\", \"I'd\", \"I'll\", \"I'm\", \"I've\", \"isn't\", \"it'd\",\n",
        "                    \"it'll\", \"it's\", \"let's\", \"mayn't\", \"may've\", \"mightn't\", \n",
        "                    \"might've\", \"mustn't\", \"must've\", \"needn't\", \"o'clock\", \"ol'\",\n",
        "                    \"oughtn't\", \"shan't\", \"she'd\", \"she'll\", \"she's\", \"should've\",\n",
        "                    \"shouldn't\", \"somebody's\", \"someone's\", \"something's\", \"that'll\",\n",
        "                    \"that're\", \"that's\", \"that'd\", \"there'd\", \"there're\", \"there's\", \n",
        "                    \"these're\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this's\",\n",
        "                    \"those're\", \"tis\", \"twas\", \"twasn't\", \"wasn't\", \"we'd\", \"we'd've\",\n",
        "                    \"we'll\", \"we're\", \"we've\", \"weren't\", \"what'd\", \"what'll\", \n",
        "                    \"what're\", \"what's\", \"what've\", \"when's\", \"where'd\", \"where're\",\n",
        "                    \"where's\", \"where've\", \"which's\", \"who'd\", \"who'd've\", \"who'll\",\n",
        "                    \"who're\", \"who's\", \"who've\", \"why'd\", \"why're\", \"why's\", \"won't\",\n",
        "                    \"would've\", \"wouldn't\", \"y'all\", \"you'd\", \"you'll\", \"you're\", \n",
        "                    \"you've\", \"'s\", \"s\"\n",
        "                     ]\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "custom_stopwords = text.ENGLISH_STOP_WORDS.union(eng_contractions)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJf0ZbmmZdZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "def tokenize_and_stem(text, do_stem=True):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    \n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "            \n",
        "    # stem filtered tokens\n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "    \n",
        "    if do_stem:\n",
        "        return stems\n",
        "    else:\n",
        "        return filtered_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiNrntw7bCR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# not super pythonic, no, not at all.\n",
        "# use extend so it's a big flat list of vocab\n",
        "totalvocab_stemmed = []\n",
        "totalvocab_tokenized = []\n",
        "for i in range(0, 80):\n",
        "    body_text = features.iloc[i , 2]\n",
        "    allwords_stemmed = tokenize_and_stem(body_text)\n",
        "    totalvocab_stemmed.extend(allwords_stemmed)\n",
        "    \n",
        "    allwords_tokenized = tokenize_and_stem(body_text, False)\n",
        "    totalvocab_tokenized.extend(allwords_tokenized)  \n",
        "  \n",
        "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWc66FByfRij",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "dc7c1d1e-92f3-41df-a2d0-8ec7e9b7412c"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#define vectorizer parameters\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
        "                                 min_df=0.2, stop_words=custom_stopwords,\n",
        "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(features['Body']) #fit the vectorizer to synopses\n",
        "\n",
        "print(tfidf_matrix.shape)\n",
        "\n",
        "terms = tfidf_vectorizer.get_feature_names()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'m\", 'abov', 'afterward', 'ai', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'ca', 'cri', 'dare', 'describ', 'did', 'doe', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'gon', 'got', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'let', 'll', 'mani', 'meanwhil', 'moreov', \"n't\", 'na', 'need', 'nobodi', 'noon', 'noth', 'nowher', 'ol', 'onc', 'onli', 'otherwis', 'ought', 'ourselv', 'perhap', 'pleas', 'sever', 'sha', 'sinc', 'sincer', 'sixti', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'ta', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 've', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'wo', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(148, 240)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K10sGl89faN-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2bdbea21-6a40-4741-91a9-2b1d52240aca"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import math\n",
        "\n",
        "num_clusters = int(math.sqrt(features.shape[0] / 2) * 1.5)\n",
        "\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "\n",
        "km.fit(tfidf_matrix)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
              "       n_clusters=12, n_init=10, n_jobs=None, precompute_distances='auto',\n",
              "       random_state=None, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbPkN2R_fiKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clusters = km.labels_.tolist()\n",
        "\n",
        "features['cluster'] = clusters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMQhzuRDfxqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9be7ddb9-458c-49dd-d09e-724ebcdb4db6"
      },
      "source": [
        "print(\"Top terms per cluster:\")\n",
        "print()\n",
        "\n",
        "#sort cluster centers by proximity to centroid\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
        "\n",
        "for i in range(num_clusters):\n",
        "    print(\"Cluster %d words:\" % i, end='')\n",
        "    \n",
        "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
        "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
        "    print() #add whitespace\n",
        "    print() #add whitespace\n",
        "    \n",
        "    print(\"Cluster %d titles:\" % i, end='')\n",
        "    print()\n",
        "    for title in features[features['cluster'] == i]['Title'].values.tolist():\n",
        "        print(' - %s' % title)\n",
        "    print() #add whitespace\n",
        "    print() #add whitespace\n",
        "    \n",
        "print()\n",
        "print()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top terms per cluster:\n",
            "\n",
            "Cluster 0 words: region, managers, u, high, asset, operate,\n",
            "\n",
            "Cluster 0 titles:\n",
            " - nan\n",
            " - nan\n",
            "\n",
            "\n",
            "Cluster 1 words: br, br, stores, retailer, said, companies,\n",
            "\n",
            "Cluster 1 titles:\n",
            " - Scrapped Rite Aid-Albertsons Merger, Mattress Firm’s Financial Struggles Could Lead to Thousands of Vacancies\n",
            " - Cosmetics Retailers Ramp Up Plans to Open Stores, Distribution Hubs as Amazon Sparks Battle for Beauty\n",
            " - TJX Companies Plans to Add 2,000 More Stores\n",
            " - Lowe’s to Close All 99 Orchard Hardware Stores\n",
            " - A Moving Target: Smaller Stores, New Fulfillment Options Power Strong Performance\n",
            " - Ross Shoots for 3,000 Stores\n",
            " - Authentic Brands Group Makes $35 Million Bid for Brookstone Ahead of Auction\n",
            " - Tiffany & Co. to Renovate Flagship New York Store, Remodel Outlets Across the Country\n",
            " - American Eagle Joins Retailers Opening More Stores, Many in New Markets\n",
            " - Coca-Cola to Buy UK's Costa for $5.1 Billion to Add Global Coffee Brand\n",
            " - Landlords Could Seize Opportunity If Papa John’s Shuts Stores\n",
            " - Sears Holdings Store Closings to Slow in Coming Months\n",
            " - Price War for Brookstone Heats Up With $56.4M Bid \n",
            " - L Brands to Close All 23 Henri Bendel Stores\n",
            " - AT&T Expands Footprint with 1,000 Phone Stores - and a Coffee Shop\n",
            " - Michael Kors Plans New Stores Following $2.12 Billion Purchase of Luxury Apparel Retailer Versace \n",
            " - MTN Retail Advisors Launches Tenant Representation Services Team in Salt Lake City\n",
            "\n",
            "\n",
            "Cluster 2 words: firms, design, unit, companies, offering, projections,\n",
            "\n",
            "Cluster 2 titles:\n",
            " - Amazon Courtship Continues - Behind Closed Doors\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            "\n",
            "\n",
            "Cluster 3 words: hover, decoration, hover, hover, text, text,\n",
            "\n",
            "Cluster 3 titles:\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            "\n",
            "\n",
            "Cluster 4 words: br, br, said, companies, developments, offices,\n",
            "\n",
            "Cluster 4 titles:\n",
            " - Complete Flooring Supply Corp. to Create 100 Jobs in Calhoun, GA, with New Manufacturing Plant\n",
            " - Robust Leasing Drives Strong Midyear Results by Top Brokerage Firms\r\n",
            "\n",
            " - WeWork: Meet Me in the Middle\n",
            " - HQ2: Where Amazon Goes, Other Companies Follow\n",
            " - Activist Investor Litt Boosts Stake in Mack-Cali \n",
            " - Colliers Broker Joins MMG Equity Partners in Bet on South Florida Shopping Centers\n",
            " - GlenStar Gets New Partner for Class A Office Play in Suburban Chicago\n",
            " - Pair of Australian Investment Firms Make $2 Billion Bet on U.S. Apartments\n",
            " - Sears Closings Pick Up Speed With Another 46 Stores Set to Go Dark\n",
            " - Record Demand for U.S. Hotels Fuels Pebblebrook Bidding for LaSalle Hotel Properties\r\n",
            "\r\n",
            "\n",
            " - Wallace Elevated to President of CBRE's Mountain-Northwest Division\n",
            " - Locally Based Firm Picks Up The Madison in Seattle's First Hill for $13 Million\n",
            " - LaGanke Returns to Cushman & Wakefield in Phoenix\n",
            " - Google Moving Forward with $600 Million Expansion of South Carolina Data Center After County Approves Incentive Package\n",
            " - Developers' New Plans Call for Chicago Towers as Tall as 80 Floors\n",
            " - Mortenson Construction Plans Further Expansion Into New Business Lines\n",
            " - WeWork Says It's Manhattan's Top User of Office Space \r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\n",
            " - Potential for Backlash Mounts as Amazon HQ2 Fatigue Sets In\n",
            " - Exclusive: BBG Acquires Austin, Texas-Based Appraisal and Advisory Firm\r\n",
            "\n",
            " - Avison Young Launches Life Science Practice Group\n",
            " - Ex-Cushman & Wakefield Executive Files $30 Million Race, Gender Discrimination Lawsuit\n",
            " - Amazon's Proposed HQ2 Isn't The Only Recipient of Tax Break Offers \n",
            " - Arby’s Owner Could Serve Up More Sonic Restaurants After Buying Drive-In Chain for $2.3 Billion\n",
            "\n",
            "\n",
            "Cluster 5 words: cities, headquarters, jobs, site, billion, state,\n",
            "\n",
            "Cluster 5 titles:\n",
            " - Newmark Knight Frank Boosts National Appraisal Platform with Deal to Roll Up IRR Affiliate Offices in NYC/NJ, Five Other US Markets\n",
            " - Amazon Outgrows Seattle: Opens Search for Second HQ City in North America\n",
            " - For Amazon’s Second HQ’s Search, Bigger May Be Better\n",
            " - Last Call for Amazon HQ2: Today is Deadline for Communities to Submit Bids for Landing Co-Headquarters\n",
            " - Hundreds of Localities Fortify Their Amazon HQ2 Bids with Hefty Financial Incentives\n",
            " - Amazon Narrows HQ2 Search to 20 Markets\n",
            " - A Look at the Six Newark Sites that Make Up New Jersey's Amazon HQ2 Pitch\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            "\n",
            "\n",
            "Cluster 6 words: br, br, nbsp, style=, new, said,\n",
            "\n",
            "Cluster 6 titles:\n",
            " - Deutsche Finance America Launches with Denver Headquarters\n",
            " - Minneapolis-based Architecture, Engineering Firm Expands to Dallas-Fort Worth\n",
            " - VF Corp. Relocating Corporate HQ to Denver, Bringing 800 Jobs\n",
            " - McDonald's Outlines $6 Billion National Plan to Build, Renovate Restaurants\n",
            " - Workbar Hires Cushman & Wakefield to Aid in Greater Boston, National Expansion\n",
            " - Seattle Startups Created by Former Amazon Staff Shows What May Await an HQ2 Region\n",
            " - Arby's Parent to Create 1,100 Jobs at New Atlanta-Area Headquarters\n",
            " - Atlanta's Secret 'Project Yogurt' Lures Starbucks Regional Operation\n",
            " - Marcus & Millichap Continues Canadian Expansion\n",
            " - Pebblebrook to Sell $1.72 Billion in Hotel Assets to Help Cover $5.2 Billion Purchase Price of LaSalle\n",
            " - WeWork Executive Says Shared Office Provider Is Ready for Any Downturn\n",
            " - Fayetteville, North Carolina, Watches Rivers Rise Amid Curfew, Spotty Power, Boarded Up Businesses\n",
            " - Cousins Properties Names Connolly CEO\n",
            " - Willis Tower Revamp Looks Back to Sears for Inspiration\n",
            " - Big Scoop: Museum of Ice Cream Aims to Sprinkle Stores Across the World\n",
            " - Ivanhoé Cambridge Acquires Callahan Capital Properties\n",
            " - Chipotle to Expand in Ohio As Part of Corporate Restructuring \n",
            " - Madison Marquette Drops PMRG Name\n",
            " - WeWork Labs Plans First Texas Location in Downtown Dallas\n",
            " - Amazon Invests in Prefab Housing Builder\n",
            "\n",
            "\n",
            "Cluster 7 words: managers, business, commercial, san, executives, firms,\n",
            "\n",
            "Cluster 7 titles:\n",
            " - South Florida to Amazon: Come for the Lifestyle and Location, Not the 'Corporate Welfare'\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            "\n",
            "\n",
            "Cluster 8 words: stores, brands, close, opens, sales, retailer,\n",
            "\n",
            "Cluster 8 titles:\n",
            " - The Amazon Effect: What Would Happen to Apartment Rents if Your City is Picked for HQ2?\n",
            " - Political Tensions Facing Amazon HQ2 Finalists Emerge in New Jersey \n",
            " - AccorHotels to Acquire 85% Stake in 21c Museum Hotels\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            "\n",
            "\n",
            "Cluster 9 words: brands, headquarters, hover, text, hover, decoration,\n",
            "\n",
            "Cluster 9 titles:\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            "\n",
            "\n",
            "Cluster 10 words: properties, holding, values, asset, shares, offering,\n",
            "\n",
            "Cluster 10 titles:\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            " - nan\n",
            "\n",
            "\n",
            "Cluster 11 words: br, br, real, real, estate, partners,\n",
            "\n",
            "Cluster 11 titles:\n",
            " - Marcus & Millichap Recruits KeyBank Executive as New Leader of Capital Markets Division\n",
            " - Angelo Gordon Raises $1.9 Billion for its Latest Real Estate Fund\n",
            " - CBRE Buys Peloton Operation in San Antonio\n",
            " - Edge Commercial to Grow Capital Markets Practice, Expand Advisory Platform with New D.C. Office\n",
            " - TIAA Buys Stake in Three Large Malls From Brookfield \n",
            " - CBRE Group Buys New England Joint-Venture Partner\n",
            " - Lampert’s ESL Hedge Fund Offers Sears Holdings a Plan to Reduce Debt\n",
            " - Dakota Pacific Acquires Parley's Partners in Salt Lake City\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}